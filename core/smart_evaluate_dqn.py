import argparse
import csv
from smart_util import Util
from smart_agent import Agent
from smart_dqn import DQN
from smart_environment import Environment
from smart_environment_v2 import Environment2
from smart_query_estimator import Query_Estimator
import torch


###########################################################
#  smart_evaluate_dqn.py
#
#  -d  / --dimension       dimension: dimension of the queries. Default: 3
#  -nj / --num_join        number of join methods. Default: 1
#  -mf / --model_file      input file that holds trained dqn model
#  -lf / --labeled_file    input file that holds labeled queries for evaluation
#  -uc / --unit_cost       time (second) to collect selectivity value for one condition
#  -tb / --time_budget     time (second) for a query to be viable
#  -ef / --evaluated_file  output file that holds the evaluated queries result
#  -v  / --version         version of DQN and environment to use
#  -dbg  / --debug         debug query id
#  ** Only required when version = 1/2:
#  -llsf / --list_labeled_sel_file  list of labeled_sel_queries files for different sample sizes
#  -lsqf / --list_sel_query_file    list of sel_queries files for different sample sizes
#  -scf  / --sel_costs_file         input file that holds sel queries costs for different sample sizes
#  -qmp  / --qe_model_path          input path to load the models used by Query Estimator
#  -sp   / --sample_pointer         pointer to the sample size to use for the query_estimator. Default: 0
#
# Dependencies:
#   python3.7 & pip: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/eb-cli3-install-linux.html
#   pip install torch
#
###########################################################

# Evaluate DQN
#
# @param - dimension: dimension of the queries
# @param - dqn_model: DQN object, DQN model generated by smart_train_dqn.py
# @param - labeled_queries: [list of query objects], each query object being
#          {id, time_0, time_1, ..., time_(2**d-1)}
# @param - unit_cost: float, time (second) to collect selectivity value for one condition
# @param - time_budget: float, time (second) for a query to be viable
# @param - version: int, version of DQN and environment to use
#
# * Only valid when version == 1/2:
# @param - samples_labeled_sel_queries: [list of [list of sel queries times]], each inside list being
#          [{id, time_sel_1, time_sel_2, ..., time_sel_(2**d-1)}]
#          * the outside list is ordered by the sample sizes ascending (e.g., 5k, 50k, 500k)
# @param - samples_query_sels: [list of [list of query_sels]], each inside list being
#          [{id, sel_1, sel_2, ..., sel_(2**d-1)}]
#          * the outside list is ordered by the sample sizes ascending (e.g., 5k, 50k, 500k)
# @param - samples_sel_queries_costs: [list of [list of sel query cost]], each inside list being
#          [cost(sel_1), cost(sel_2), ..., cost(sel_(2**d-1))]
#          * the outside list is ordered by the sample sizes ascending (e.g., 5k, 50k, 500k)
# @param - query_estimator: object, Query_Estimator class instance
# @param - sample_pointer: int, [0~2], pointer to the sample size to use for the query_estimator. Default: 0
# @param - num_of_joins: int, number of join methods in hints set.
#
# @return - (list of evaluated query objects, win_rate), each query object being
#           {id, planning_time, querying_time, total_time, win(1/0), plans_tried(x_x_x_x), reason}
def evaluate_dqn(dimension,
                 dqn_model,
                 labeled_queries,
                 unit_cost,
                 time_budget,
                 samples_labeled_sel_queries=[],
                 samples_query_sels=[],
                 samples_sel_queries_costs=None,
                 query_estimator=None,
                 sample_pointer=0,
                 version='1',
                 debug_qid=-1,
                 num_of_joins=1):

    # set DQN model as the policy_net for agent
    policy_net = dqn_model

    # init objects
    # version 0
    if version == '0':
        env = Environment(dimension, labeled_queries, unit_cost, time_budget, num_of_joins)
        agent = Agent(dimension, num_of_joins)
    # version 1/2
    elif version == '1' or version == '2':
        env = Environment2(dimension,
                           labeled_queries,
                           samples_labeled_sel_queries,
                           samples_query_sels,
                           samples_sel_queries_costs,
                           query_estimator,
                           time_budget,
                           sample_pointer=sample_pointer,
                           num_of_joins=num_of_joins)
        agent = Agent(dimension, num_of_joins)
    # default
    else:
        env = Environment(dimension, labeled_queries, unit_cost, time_budget, num_of_joins)
        agent = Agent(dimension, num_of_joins)
        print("Invalid version " + str(version) + "!")
        exit(0)

    # evaluate labeled queries one by one
    evaluated_queries = []
    win_rate = 0.0
    for query in labeled_queries:
        qid = query["id"]
        env.reset(qid)
        state = env.get_state()
        agent.reset()
        plans_tried = []

        # ++++++++++ DEBUG ++++++++++ #
        if 0 <= debug_qid < qid:
            break
        if qid == debug_qid:
            print("[DEBUG]    query [" + str(qid) + "]")
        # ---------- DEBUG ---------- #

        plan = 0
        while not env.done:
            # ++++++++++ DEBUG ++++++++++ #
            remain_budget = time_budget - state.get_elapsed_time()
            # ---------- DEBUG ---------- #
            action = agent.decide_action(state, policy_net)
            # action = agent.decide_action(state, policy_net, random_ai=True)
            plan = action + 1
            plans_tried.append(plan)
            reward = env.take_action(plan)
            state = env.get_state()

            # ++++++++++ DEBUG ++++++++++ #
            if qid == debug_qid:
                if version == '1' or version == '2':
                    estimate_time = state.get_estimate_times()[plan - 1]
                    real_cost = -reward
                    print("[DEBUG]        Remain budget: " + str(remain_budget) +
                          ", estimate plan [" + str(plan) +
                          "], estimate time: " + str(estimate_time) +
                          ", real cost: " + str(real_cost) + ".")
            # ---------- DEBUG ---------- #

        planning_time = state.get_elapsed_time()
        querying_time = env.get_query_time()
        total_time = planning_time + querying_time
        if total_time <= time_budget:
            win = True
            win_rate += 1
        else:
            win = False
        reason = env.get_done_reason()

        # ++++++++++ DEBUG ++++++++++ #
        if qid == debug_qid:
            print("[DEBUG]    planning_time: " + str(planning_time) +
                  ", querying_time: " + str(querying_time) +
                  ", total_time: " + str(total_time) +
                  ", win: " + str(win) +
                  ", reason: " + str(reason))
        # ---------- DEBUG ---------- #

        evaluated_queries.append(
            {"id": qid,
             "planning_time": planning_time,
             "querying_time": querying_time,
             "total_time": total_time,
             "win": (1 if win else 0),
             "plans_tried": "_".join(str(x) for x in plans_tried),
             "reason": reason
             }
        )

    win_rate = win_rate / len(labeled_queries)

    return evaluated_queries, win_rate


if __name__ == "__main__":

    # parse arguments
    parser = argparse.ArgumentParser(description="Evaluate DQN.")
    parser.add_argument("-d", "--dimension",
                        help="dimension: dimension of the queries. Default: 3",
                        type=int, required=False, default=3)
    parser.add_argument("-nj", "--num_join", help="num_join: number of join methods. Default: 1", 
                        required=False, type=int, default=1)
    parser.add_argument("-mf", "--model_file",
                        help="model_file: input file that holds trained dqn model",
                        type=str, required=True)
    parser.add_argument("-lf", "--labeled_file",
                        help="labeled_file: input file that holds labeled queries for evaluation",
                        type=str, required=True)
    parser.add_argument("-uc", "--unit_cost",
                        help="unit_cost: time (second) to collect selectivity value for one condition",
                        type=float, required=False, default=0.05)
    parser.add_argument("-tb", "--time_budget",
                        help="time_budget: time (second) for a query to be viable",
                        type=float, required=True)
    parser.add_argument("-ef", "--evaluated_file",
                        help="evaluated_file: output file that holds the evaluated queries result",
                        type=str, required=True)
    parser.add_argument("-v", "--version",
                        help="version: version of DQN and environment to use. Default: '1'",
                        type=str, required=False, default='1')
    parser.add_argument("-dbg", "--debug",
                        help="debug: debug query id. Default: -1",
                        type=int, required=False, default=-1)
    parser.add_argument("-llsf", "--list_labeled_sel_file",
                        help="list_labeled_sel_file: list of labeled_sel_queries files for different sample sizes",
                        action='append', required=False, default=[])
    parser.add_argument("-lsqf", "--list_sel_query_file",
                        help="list_sel_query_file: list of sel_queries files for different sample sizes",
                        action='append', required=False, default=[])
    parser.add_argument("-scf", "--sel_costs_file",
                        help="sel_costs_file: input file that holds sel queries costs for different sample sizes",
                        type=str, required=False, default=None)
    parser.add_argument("-qmp", "--qe_model_path",
                        help="qe_model_path: input path to load the models used by Query Estimator",
                        type=str, required=False, default=None)
    parser.add_argument("-sp", "--sample_pointer",
                        help="sample_pointer: pointer to the sample size to use for the query_estimator. Default: 0",
                        type=int, required=False, default=0)
    args = parser.parse_args()

    dimension = args.dimension
    num_of_joins = args.num_join
    dqn_model_file = args.model_file
    labeled_queries_file = args.labeled_file
    unit_cost = args.unit_cost
    time_budget = args.time_budget
    evaluated_queries_file = args.evaluated_file
    version = args.version
    debug_qid = args.debug
    sample_pointer = args.sample_pointer

    # load labeled queries into memory
    labeled_queries = Util.load_labeled_queries_file(dimension, labeled_queries_file, num_of_joins)

    # For version = 1/2
    if version == '1' or version == '2':
        list_labeled_sel_file = args.list_labeled_sel_file
        list_sel_query_file = args.list_sel_query_file
        sel_costs_file = args.sel_costs_file
        qe_model_path = args.qe_model_path

        # assert required parameters
        if len(list_labeled_sel_file) == 0:
            print("-llsf / --list_labeled_sel_file is required when --version is 2!")
            exit(0)
        if len(list_sel_query_file) == 0:
            print("-lsqf / --list_sel_query_file is required when --version is 2!")
            exit(0)
        if sel_costs_file is None:
            print("-scf / --sel_costs_file is required when --version is 2!")
            exit(0)
        if qe_model_path is None:
            print("-qmp / --qe_model_path is required when --version is 2!")
            exit(0)
        if len(list_labeled_sel_file) != len(list_sel_query_file):
            print("lengths of list_labeled_sel_file & list_sel_query_file must be the same when --version is 2!")
            exit(0)

        samples_labeled_sel_queries = Util.load_labeled_sel_queries_files(dimension, list_labeled_sel_file)
        samples_query_sels = Util.load_queries_sels_files(dimension, list_sel_query_file)
        samples_sel_queries_costs = Util.load_sel_queries_costs_file(dimension, sel_costs_file)

        # new a Query Estimator
        query_estimator = Query_Estimator(dimension, num_of_joins)
        # load Query Estimator models from files
        query_estimator.load(qe_model_path)
    else:
        samples_labeled_sel_queries = []
        samples_query_sels = []
        samples_sel_queries_costs = None
        query_estimator = None

    # load DQN model
    # version 0
    if version == '0':
        dqn_model = DQN(dimension, num_of_joins)
    # version 1/2
    elif version == '1' or version == '2':
        dqn_model = DQN(dimension, num_of_joins)
    # default
    else:
        dqn_model = DQN(dimension, num_of_joins)
        print("Invalid version " + str(version) + "!")
        exit(0)
    dqn_model.load_state_dict(torch.load(dqn_model_file))
    dqn_model.eval()
    print("DQN model loaded into memory.")

    # evaluated DQN
    (evaluated_queries, win_rate) = evaluate_dqn(dimension,
                                                 dqn_model,
                                                 labeled_queries,
                                                 unit_cost,
                                                 time_budget,
                                                 samples_labeled_sel_queries=samples_labeled_sel_queries,
                                                 samples_query_sels=samples_query_sels,
                                                 samples_sel_queries_costs=samples_sel_queries_costs,
                                                 query_estimator=query_estimator,
                                                 sample_pointer=sample_pointer,
                                                 version=version,
                                                 debug_qid=debug_qid,
                                                 num_of_joins=num_of_joins)

    # in debug mode, do not output evaluated queries
    if debug_qid == -1:
        # output evaluated queries to console
        print("======== Evaluation of DQN ========")
        print(labeled_queries_file)
        print("-----------------------------------")
        print("qid,    planning_time,    querying_time,    total_time,    win,    plans_tried,    reason")
        for query in evaluated_queries:
            print(str(query["id"]) + ",    " + str(query["planning_time"]) + ",    " +
                  str(query["querying_time"]) + ",    " + str(query["total_time"]) + ",    " + str(query["win"]) +
                  ",    " + query["plans_tried"] + ",    " + query["reason"])

        print("-----------------------------------")
        print("win rate: " + str(win_rate))
        print("===================================")

        # output evaluated queries to file
        Util.dump_evaluated_queries_file(evaluated_queries_file, evaluated_queries)
        print("evaluated queries saved to file [" + evaluated_queries_file + "].")

