from datetime import timedelta
from postgresql import PostgreSQL
import random
import csv
import config
import argparse


###########################################################
#  smart_generate_queries_nyc.py
#
#  -n   / --number          number of queries to be generated
#  -of  / --out_file        output file to hold the queries
#  -a   / --append          append generated queries to out_file if it exists
#  -sid / --start_id        starting id for generated queries, default: 0
#
# Dependencies:
#   python3.7 & pip: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/eb-cli3-install-linux.html
#
# Requirements:
#   1) PostgreSQL 9.6+
#   2) Schema:
#        nyc_500m (
#            id                        bigint primary key,
#        [x] pickup_datetime           timestamp,
#            dropoff_datetime          timestamp,
#            passenger_count           int,
#        [x] trip_distance             numeric,
#        [x] pickup_coordinates        point,
#            dropoff_coordinates       point,
#            payment_type              text,
#            fare_amount               numeric,
#            extra                     numeric,
#            mta_tax                   numeric,
#            tip_amount                numeric,
#            tolls_amount              numeric,
#            improvement_surcharge     numeric,
#            total_amount              numeric
#        );
#
# Output:
#   format (csv):
#     id, start_time, end_time, trip_distance_start, trip_distance_end, lng0, lat0, lng1, lat1
#     ...
#   example:
#     0, 2010-11-01 00:00:00, 2010-12-01 00:00:00, 1.0, 2.0, -73.956778, 40.767749, -73.965957, 40.765231
#     1, 2010-01-30 23:31:00, 2010-01-31 23:46:12, 0.7, 5.9, -73.996117, 40.74119, -73.981511, 40.3763931
#     ...
#
###########################################################


database_config = config.database_configs["postgresql"]
dataset = config.datasets["nyc"]

# Constants
MAX_TEMPORAL_ZOOM_LEVEL = dataset.max_pickup_datetime_zoom
MAX_NUMERICAL_ZOOM_LEVEL = dataset.max_trip_distance_zoom
MAX_SPATIAL_ZOOM_LEVEL = dataset.max_pickup_coordinates_zoom


# TODO - Move this function inside each DB's adapter
# sample trip randomly from the database
# @return - [
#              (datetime.datetime(2010, 01, 30, 23, 31, 00), 5.9, -73.996117, 40.763931),
#               ...
#           ]
def sample_random_trip(_number_of_trips, _total_number_of_trips):
    sample_percent = _number_of_trips * 100.0 / _total_number_of_trips
    # enlarge threshold by 20% to ensure enough candidates generated by the db
    sample_percent = sample_percent * 1.2
    print("sample percent = ", sample_percent)
    sql = "select pickup_datetime, trip_distance, pickup_coordinates[0], pickup_coordinates[1] " \
          "from " + dataset.table + " " \
          "tablesample bernoulli (" + str(sample_percent) + ")"
    postgresql = PostgreSQL(
        database_config.hostname,
        database_config.username,
        database_config.password,
        dataset.database
    )
    results = postgresql.query(sql)
    postgresql.close()
    return results


if __name__ == "__main__":

    # parse arguments
    parser = argparse.ArgumentParser(description="Generate benchmark queries.")
    parser.add_argument("-n", "--number",
                        help="number: number of queries to be generated", type=int, required=True)
    parser.add_argument("-of", "--out_file",
                        help="out_file: output file to hold the generated queries", required=True)
    parser.add_argument("-a", "--append",
                        help="append: append generated queries to out_file if it exists", action="store_true")
    parser.add_argument("-sid", "--start_id",
                        help="start_id: starting id for generated queries, default: 0",
                        type=int, required=False, default=0)
    args = parser.parse_args()

    number_of_queries = args.number
    out_file = args.out_file
    append = args.append
    start_id = args.start_id

    # initialize common parameters
    number_of_trips = dataset.table_size
    min_pickup_datetime_dt = PostgreSQL.string_to_datetime(dataset.min_pickup_datetime)
    max_pickup_datetime_dt = PostgreSQL.string_to_datetime(dataset.max_pickup_datetime)
    max_number_of_seconds = (max_pickup_datetime_dt - min_pickup_datetime_dt).total_seconds()
    max_trip_distance_range = dataset.max_trip_distance - dataset.min_trip_distance
    max_width = dataset.max_pickup_coordinates_lng - dataset.min_pickup_coordinates_lng
    max_height = dataset.max_pickup_coordinates_lat - dataset.min_pickup_coordinates_lat

    print("start generating queries ...")

    # 1. randomly sample trips from the database
    sample_trips = sample_random_trip(number_of_queries, number_of_trips)

    print("sampled ", len(sample_trips), " trips from ", number_of_trips, " trips.")

    generated_queries = []

    # 2. for each sample trip, generate a random query
    id = start_id
    # each trip is a tuple with 4 elements
    #   [datetime(pickup_datetime), float(trip_distance), float(pickup_coordinates_lng), float(pickup_coordinates_lat)]
    for trip in sample_trips:
        # 1) pick pickup_datetime as the start of the query's pickup_datetime range
        pickup_datetime_range_start_dt = trip[0]
        # generate a random zoom level (within [0, MAX_TEMPORAL_ZOOM_LEVEL]
        pickup_datetime_range_zoom_level = random.randint(0, MAX_TEMPORAL_ZOOM_LEVEL)
        # generate the time range length based on the zoom level, with minimum 5 minutes
        pickup_datetime_range_number_of_seconds = max(max_number_of_seconds // pow(2, pickup_datetime_range_zoom_level), 300)
        pickup_datetime_range_timedelta = timedelta(seconds=pickup_datetime_range_number_of_seconds)
        # set the query time range with the start date and range length
        pickup_datetime_range_end_dt = min(max_pickup_datetime_dt, pickup_datetime_range_start_dt + pickup_datetime_range_timedelta)
        pickup_datetime_range_start_str = PostgreSQL.datetime_to_string(pickup_datetime_range_start_dt)
        pickup_datetime_range_end_str = PostgreSQL.datetime_to_string(pickup_datetime_range_end_dt)

        # 2) pick trip_distance as the center of the trip_distance range
        trip_distance_range_center = float(trip[1])
        # generate a random zoom level (within [0, MAX_NUMERICAL_ZOOM_LEVEL]
        trip_distance_range_zoom_level = random.randint(0, MAX_NUMERICAL_ZOOM_LEVEL)
        trip_distance_range = max_trip_distance_range / pow(2, trip_distance_range_zoom_level)
        trip_distance_range_start = max(trip_distance_range_center - trip_distance_range / 2, dataset.min_trip_distance)
        trip_distance_range_end = min(trip_distance_range_center + trip_distance_range / 2, dataset.max_trip_distance)

        # 3) pick pickup_coordinates as the center of the spatial range
        spatial_range_center = (trip[2], trip[3])
        # generate a random zoom level (within [0, MAX_SPATIAL_ZOOM_LEVEL]
        spatial_range_zoom_level = random.randint(0, MAX_SPATIAL_ZOOM_LEVEL)
        # generate the spatial range (width, height) based on the zoom level
        spatial_range_width = max_width / pow(2, spatial_range_zoom_level)
        spatial_range_height = max_height / pow(2, spatial_range_zoom_level)
        # set the query spatial range with the above center and range width & height
        spatial_range_lng0 = float(spatial_range_center[0]) - spatial_range_width/2
        spatial_range_lat0 = float(spatial_range_center[1]) - spatial_range_height/2
        spatial_range_lng1 = float(spatial_range_center[0]) + spatial_range_width/2
        spatial_range_lat1 = float(spatial_range_center[1]) + spatial_range_height/2

        generated_query = [
            id,
            pickup_datetime_range_start_str,
            pickup_datetime_range_end_str,
            trip_distance_range_start,
            trip_distance_range_end,
            spatial_range_lng0,
            spatial_range_lat0,
            spatial_range_lng1,
            spatial_range_lat1
        ]
        generated_queries.append(generated_query)
        print(",".join(str(x) for x in generated_query))
        id += 1

    print("generated ", len(generated_queries), " queries.")
    # write out the generated queries to csv file
    file_mode = "w"
    if append:
        file_mode = "a"
    with open(out_file, file_mode) as csv_out:
        csv_writer = csv.writer(csv_out, delimiter=',', quotechar='"', quoting=csv.QUOTE_MINIMAL)
        for query in generated_queries:
            csv_writer.writerow(query)
    print("wrote to csv file ", out_file, ".")

