from datetime import timedelta
import nltk
from nltk.corpus import stopwords
from postgresql import PostgreSQL
import random
import csv
import config
import argparse


###########################################################
#  smart_generate_queries_twitter.py
#
#  -d   / --dimension       dimension of the queries. Default: 3
#  -n   / --number          number of queries to be generated
#  -of  / --out_file        output file to hold the queries
#  -a   / --append          append generated queries to out_file if it exists
#  -sid / --start_id        starting id for generated queries, default: 0
#
# Dependencies:
#   python3.7 & pip: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/eb-cli3-install-linux.html
#   pip install requests
#   pip install nltk
#   python3 -c 'import nltk; nltk.download("stopwords")'
#
# Requirements:
#   TODO - generalize to other DBs
#   1) PostgreSQL 9.6+
#   2) Schema:
#        tweets (
#          id                      int64,
#      [x] create_at               timestamp,
#      [x] text                    text,
#      [x] coordinate              point,
#          user_description        text,
#          user_create_at          date,
#      [x] user_followers_count    int,
#          user_friends_count      int,
#      [x] user_statues_count      int
#        )
#
# Output:
#   format (csv):
#     id, keyword, start time, end time, lng0, lat0, lng1, lat1[, user_followers_count_start, user_followers_count_end,
#       user_statues_count_start, user_statues_count_end]
#     ...
#   example:
#     0, hurricane, 2015-12-01 00:00:00, 2016-01-01 00:00:00, -77.119759, 38.791645, -76.909395, 38.99511[, 10, 100,
#       20, 25]
#     1, love, 2015-12-01 00:00:00, 2015-12-02 00:00:00, -124.409591, 32.534156, -114.131211, 42.009518[, 1000, 10000,
#       3000, 4000]
#     ...
#
###########################################################


database_config = config.database_configs["postgresql"]
dataset = config.datasets["twitter"]

# Constants
MAX_TEMPORAL_ZOOM_LEVEL = dataset.max_temporal_zoom
MAX_SPATIAL_ZOOM_LEVEL = dataset.max_spatial_zoom
MAX_USER_FOLLOWERS_COUNT_ZOOM_LEVEL = dataset.max_user_followers_count_zoom
MAX_USER_STATUES_COUNT_ZOOM_LEVEL = dataset.max_user_statues_count_zoom
nltk.download('stopwords')
EN_STOPWORDS = set(stopwords.words('english'))


# TODO - Move this function inside each DB's adapter
# sample tweets randomly from the database
# @return - [
#              (datetime.datetime(2015, 11, 18, 1, 32, 17), 'Elevate your Smoothness Factor', -74.0243074, 40.63210652,
#               300, 2000),
#               ...
#           ]
def sample_random_tweets(_number_of_tweets, _total_number_of_tweets):
    sample_percent = _number_of_tweets * 100.0 / _total_number_of_tweets
    # enlarge threshold by 20% to ensure enough candidates generated by the db
    sample_percent = sample_percent * 1.2
    print("sample percent = ", sample_percent)
    sql = "select create_at, text, coordinate[0], coordinate[1], user_followers_count, user_statues_count " \
          "from " + dataset.table + " " \
          "tablesample bernoulli (" + str(sample_percent) + ")"
    postgresql = PostgreSQL(
        database_config.hostname,
        database_config.username,
        database_config.password,
        dataset.database
    )
    results = postgresql.query(sql)
    postgresql.close()
    return results


# sample one word from given string
#   - skip stop word
# @return - string if there is at least one valid
#         - None otherwise
def sample_one_word(_text):
    words = _text.split()
    candidates = []
    # put non-stop words into candidates
    for word in words:
        word = word.strip("#@").lower()
        if word.isalpha() and word not in EN_STOPWORDS:
            candidates.append(word)
    if len(candidates) > 0:
        # loop candidates in random order
        random.shuffle(candidates)
        # return the first candidate
        return candidates[0]
    # return None when all words are stop words in text
    return None


if __name__ == "__main__":

    # parse arguments
    parser = argparse.ArgumentParser(description="Generate benchmark queries for Twitter.")
    parser.add_argument("-d", "--dimension", help="dimension of the queries. Default: 3",
                        type=int, required=False, default=3)
    parser.add_argument("-n", "--number",
                        help="number: number of queries to be generated", type=int, required=True)
    parser.add_argument("-of", "--out_file",
                        help="out_file: output file to hold the generated queries", required=True)
    parser.add_argument("-a", "--append",
                        help="append: append generated queries to out_file if it exists", action="store_true")
    parser.add_argument("-sid", "--start_id",
                        help="start_id: starting id for generated queries, default: 0",
                        type=int, required=False, default=0)
    args = parser.parse_args()

    dimension = args.dimension
    number_of_queries = args.number
    out_file = args.out_file
    append = args.append
    start_id = args.start_id

    # initialize common parameters
    number_of_tweets = dataset.table_size
    min_create_at_dt = PostgreSQL.string_to_datetime(dataset.min_create_at)
    max_create_at_dt = PostgreSQL.string_to_datetime(dataset.max_create_at)
    max_width = dataset.max_lng - dataset.min_lng
    max_height = dataset.max_lat - dataset.min_lat
    max_number_of_days = (max_create_at_dt - min_create_at_dt).days
    max_user_followers_count_range = dataset.max_user_followers_count - dataset.min_user_followers_count
    max_user_statues_count_range = dataset.max_user_statues_count - dataset.min_user_statues_count

    print("start generating queries ...")

    # 1. randomly sample tweets from the database
    sample_tweets = sample_random_tweets(number_of_queries, number_of_tweets)

    print("sampled ", len(sample_tweets), " tweets from ", number_of_tweets, " tweets.")

    generated_queries = []

    # 2. for each sample tweet, generate a random query
    id = start_id
    # each tweet is a tuple with 6 elements
    #   (datetime(create_at), string(text), float(lng), float(lat), int(ufc), int(usc))
    for tweet in sample_tweets:

        generated_query = [id]

        # 1) pick one word in the text
        text = tweet[1]
        word = sample_one_word(text)
        # if no proper word can be used in this tweet, skip it
        if word is None:
            continue
        generated_query.append(word)

        # 2) pick create_at as the start of the query's time range
        time_range_start_dt = tweet[0]
        # generate a random zoom level (within [0, MAX_TEMPORAL_ZOOM_LEVEL]
        time_range_zoom_level = random.randint(0, MAX_TEMPORAL_ZOOM_LEVEL)
        # generate the time range length based on the zoom level, with minimum 1 day
        time_range_number_of_days = max(max_number_of_days // pow(2, time_range_zoom_level), 1)
        time_range_timedelta = timedelta(days=time_range_number_of_days)
        # set the query time range with the start date and range length
        time_range_end_dt = min(max_create_at_dt, time_range_start_dt + time_range_timedelta)
        time_range_start_str = PostgreSQL.datetime_to_string(time_range_start_dt)
        time_range_end_str = PostgreSQL.datetime_to_string(time_range_end_dt)
        generated_query.append(time_range_start_str)
        generated_query.append(time_range_end_str)

        # 3) pick coordinate as the center of the spatial range
        spatial_range_center = (tweet[2], tweet[3])
        # generate a random zoom level (within [0, MAX_SPATIAL_ZOOM_LEVEL]
        spatial_range_zoom_level = random.randint(0, MAX_SPATIAL_ZOOM_LEVEL)
        # generate the spatial range (width, height) based on the zoom level
        spatial_range_width = max_width / pow(2, spatial_range_zoom_level)
        spatial_range_height = max_height / pow(2, spatial_range_zoom_level)
        # set the query spatial range with the above center and range width & height
        spatial_range_lng0 = float(spatial_range_center[0]) - spatial_range_width/2
        spatial_range_lat0 = float(spatial_range_center[1]) - spatial_range_height/2
        spatial_range_lng1 = float(spatial_range_center[0]) + spatial_range_width/2
        spatial_range_lat1 = float(spatial_range_center[1]) + spatial_range_height/2
        generated_query.append(spatial_range_lng0)
        generated_query.append(spatial_range_lat0)
        generated_query.append(spatial_range_lng1)
        generated_query.append(spatial_range_lat1)

        # 4) pick user_followers_count as the center of the user_followers_count range
        if dimension >= 4:
            ufc_range_center = int(tweet[4])
            # generate a random zoom level (within [0, MAX_USER_FOLLOWERS_COUNT_ZOOM_LEVEL]
            ufc_range_zoom_level = random.randint(0, MAX_USER_FOLLOWERS_COUNT_ZOOM_LEVEL)
            ufc_range = max_user_followers_count_range // pow(2, ufc_range_zoom_level)
            ufc_range_start = max(ufc_range_center - ufc_range // 2, dataset.min_user_followers_count)
            ufc_range_end = min(ufc_range_center + ufc_range // 2, dataset.max_user_followers_count)
            generated_query.append(ufc_range_start)
            generated_query.append(ufc_range_end)

        # 5) pick user_statues_count as the center of the user_statues_count range
        if dimension >= 5:
            usc_range_center = int(tweet[5])
            # generate a random zoom level (within [0, MAX_USER_STATUES_COUNT_ZOOM_LEVEL]
            usc_range_zoom_level = random.randint(0, MAX_USER_STATUES_COUNT_ZOOM_LEVEL)
            usc_range = max_user_statues_count_range // pow(2, usc_range_zoom_level)
            usc_range_start = max(usc_range_center - usc_range // 2, dataset.min_user_statues_count)
            usc_range_end = min(usc_range_center + usc_range // 2, dataset.max_user_statues_count)
            generated_query.append(usc_range_start)
            generated_query.append(usc_range_end)

        generated_queries.append(generated_query)
        print(",".join(str(x) for x in generated_query))
        id += 1

    print("generated ", len(generated_queries), " queries.")
    # write out the generated queries to csv file
    file_mode = "w"
    if append:
        file_mode = "a"
    with open(out_file, file_mode) as csv_out:
        csv_writer = csv.writer(csv_out, delimiter=',', quotechar='"', quoting=csv.QUOTE_MINIMAL)
        for query in generated_queries:
            csv_writer.writerow(query)
    print("wrote to csv file ", out_file, ".")

