from datetime import timedelta
from postgresql import PostgreSQL
import random
import csv
import config
import argparse


###########################################################
#  smart_generate_queries_tpch.py
#
#  -n   / --number          number of queries to be generated
#  -of  / --out_file        output file to hold the queries
#  -a   / --append          append generated queries to out_file if it exists
#  -sid / --start_id        starting id for generated queries, default: 0
#
# Dependencies:
#   python3.7 & pip: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/eb-cli3-install-linux.html
#
# Requirements:
#   1) PostgreSQL 9.6+
#   2) Schema:
#        lineitem_300m (
#            L_ORDERKEY       INTEGER NOT NULL,
#            L_PARTKEY        INTEGER NOT NULL,
#            L_SUPPKEY        INTEGER NOT NULL,
#            L_LINENUMBER     INTEGER NOT NULL,
#            L_QUANTITY       DECIMAL(15,2) NOT NULL,
#        [x] L_EXTENDEDPRICE  DECIMAL(15,2) NOT NULL,
#            L_DISCOUNT       DECIMAL(15,2) NOT NULL,
#            L_TAX            DECIMAL(15,2) NOT NULL,
#            L_RETURNFLAG     CHAR(1) NOT NULL,
#            L_LINESTATUS     CHAR(1) NOT NULL,
#        [x] L_SHIPDATE       DATE NOT NULL,
#            L_COMMITDATE     DATE NOT NULL,
#        [x] L_RECEIPTDATE    DATE NOT NULL,
#            L_SHIPINSTRUCT   CHAR(25) NOT NULL,
#            L_SHIPMODE       CHAR(10) NOT NULL,
#            L_COMMENT        VARCHAR(44) NOT NULL
#        )
#
# Output:
#   format (csv):
#     id, extended_price_start, extended_price_end, ship_date_start, ship_date_end, receipt_date_start, receipt_date_end
#     ...
#   example:
#     0, 3800, 9000, 1995-11-01, 1995-12-01, 1996-01-01, 1996-01-31
#     1, 5000, 5100, 1997-01-30, 1997-01-31, 1997-01-30, 1997-01-31
#     ...
#
###########################################################


database_config = config.database_configs["postgresql"]
dataset = config.datasets["tpch"]

# Constants
MAX_EXTENDED_PRICE_ZOOM_LEVEL = dataset.max_extended_price_zoom
MAX_SHIP_DATE_ZOOM_LEVEL = dataset.max_ship_date_zoom
MAX_RECEIPT_DATE_ZOOM_LEVEL = dataset.max_receipt_date_zoom


# TODO - Move this function inside each DB's adapter
# sample lineitem randomly from the database
# @return - [
#              (8000.0, datetime.datetime(1995, 10, 02), datetime.datetime(1995, 11, 07)),
#               ...
#           ]
def sample_random_lineitem(_number_of_lineitems, _total_number_of_lineitems):
    sample_percent = _number_of_lineitems * 100.0 / _total_number_of_lineitems
    # enlarge threshold by 20% to ensure enough candidates generated by the db
    sample_percent = sample_percent * 1.2
    print("sample percent = ", sample_percent)
    sql = "select L_EXTENDEDPRICE, L_SHIPDATE, L_RECEIPTDATE " \
          "from " + dataset.table + " " \
          "tablesample bernoulli (" + str(sample_percent) + ")"
    postgresql = PostgreSQL(
        database_config.hostname,
        database_config.username,
        database_config.password,
        dataset.database
    )
    results = postgresql.query(sql)
    postgresql.close()
    return results


if __name__ == "__main__":

    # parse arguments
    parser = argparse.ArgumentParser(description="Generate benchmark queries.")
    parser.add_argument("-n", "--number",
                        help="number: number of queries to be generated", type=int, required=True)
    parser.add_argument("-of", "--out_file",
                        help="out_file: output file to hold the generated queries", required=True)
    parser.add_argument("-a", "--append",
                        help="append: append generated queries to out_file if it exists", action="store_true")
    parser.add_argument("-sid", "--start_id",
                        help="start_id: starting id for generated queries, default: 0",
                        type=int, required=False, default=0)
    args = parser.parse_args()

    number_of_queries = args.number
    out_file = args.out_file
    append = args.append
    start_id = args.start_id

    # initialize common parameters
    number_of_lineitems = dataset.table_size
    max_extended_price_range = dataset.max_extended_price - dataset.min_extended_price
    min_ship_date_dt = PostgreSQL.string_to_date(dataset.min_ship_date)
    max_ship_date_dt = PostgreSQL.string_to_date(dataset.max_ship_date)
    max_ship_date_range_number_of_days = (max_ship_date_dt - min_ship_date_dt).days
    min_receipt_date_dt = PostgreSQL.string_to_date(dataset.min_receipt_date)
    max_receipt_date_dt = PostgreSQL.string_to_date(dataset.max_receipt_date)
    max_receipt_date_range_number_of_days = (max_receipt_date_dt - min_receipt_date_dt).days

    print("start generating queries ...")

    # 1. randomly sample lineitems from the database
    sample_lineitems = sample_random_lineitem(number_of_queries, number_of_lineitems)

    print("sampled ", len(sample_lineitems), " trips from ", number_of_lineitems, " trips.")

    generated_queries = []

    # 2. for each sample lineitem, generate a random query
    id = start_id
    # each lineitem is a tuple with 3 elements
    #   [float(extended_price), datetime(ship_date), datetime(receipt_date)]
    for lineitem in sample_lineitems:
        # 1) pick extended_price as the center of the extended_price range
        extended_price_range_center = float(lineitem[0])
        # generate a random zoom level (within [0, MAX_EXTENDED_PRICE_ZOOM_LEVEL]
        extended_price_range_zoom_level = random.randint(0, MAX_EXTENDED_PRICE_ZOOM_LEVEL)
        extended_price_range = max_extended_price_range / pow(2, extended_price_range_zoom_level)
        extended_price_range_start = max(extended_price_range_center - extended_price_range / 2,
                                        dataset.min_extended_price)
        extended_price_range_end = min(extended_price_range_center + extended_price_range / 2,
                                      dataset.max_extended_price)

        # 2) pick ship_date as the start of the query's ship_date range
        ship_date_range_start_dt = lineitem[1]
        # generate a random zoom level (within [0, MAX_SHIP_DATE_ZOOM_LEVEL]
        ship_date_range_zoom_level = random.randint(0, MAX_SHIP_DATE_ZOOM_LEVEL)
        # generate the time range length based on the zoom level, with minimum 0 day
        ship_date_range_number_of_days = max(max_ship_date_range_number_of_days // pow(2, ship_date_range_zoom_level), 0)
        ship_date_range_timedelta = timedelta(days=ship_date_range_number_of_days)
        # set the query time range with the start date and range length
        ship_date_range_end_dt = min(max_ship_date_dt, ship_date_range_start_dt + ship_date_range_timedelta)
        ship_date_range_start_str = PostgreSQL.date_to_string(ship_date_range_start_dt)
        ship_date_range_end_str = PostgreSQL.date_to_string(ship_date_range_end_dt)

        # 3) pick receipt_date as the start of the query's receipt_date range
        receipt_date_range_start_dt = lineitem[2]
        # generate a random zoom level (within [0, MAX_RECEIPT_DATE_ZOOM_LEVEL]
        receipt_date_range_zoom_level = random.randint(0, MAX_RECEIPT_DATE_ZOOM_LEVEL)
        # generate the time range length based on the zoom level, with minimum 0 day
        receipt_date_range_number_of_days = max(max_receipt_date_range_number_of_days // pow(2, receipt_date_range_zoom_level), 0)
        receipt_date_range_timedelta = timedelta(days=receipt_date_range_number_of_days)
        # set the query time range with the start date and range length
        receipt_date_range_end_dt = min(max_receipt_date_dt, receipt_date_range_start_dt + receipt_date_range_timedelta)
        receipt_date_range_start_str = PostgreSQL.date_to_string(receipt_date_range_start_dt)
        receipt_date_range_end_str = PostgreSQL.date_to_string(receipt_date_range_end_dt)

        generated_query = [
            id,
            extended_price_range_start,
            extended_price_range_end,
            ship_date_range_start_str,
            ship_date_range_end_str,
            receipt_date_range_start_str,
            receipt_date_range_end_str
        ]
        generated_queries.append(generated_query)
        print(",".join(str(x) for x in generated_query))
        id += 1

    print("generated ", len(generated_queries), " queries.")
    # write out the generated queries to csv file
    file_mode = "w"
    if append:
        file_mode = "a"
    with open(out_file, file_mode) as csv_out:
        csv_writer = csv.writer(csv_out, delimiter=',', quotechar='"', quoting=csv.QUOTE_MINIMAL)
        for query in generated_queries:
            csv_writer.writerow(query)
    print("wrote to csv file ", out_file, ".")

